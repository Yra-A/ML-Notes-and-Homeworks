此系列笔记来源于

**Coursera上吴恩达老师的机器学习课程**

****

## 核函数 Kernels

对于非线性数据，如：

![image-20220519225423395](https://img2022.cnblogs.com/blog/1754203/202205/1754203-20220519225422763-922750227.png)

我们可以增加高阶多项式，但是计算量将会十分大

因此需要引入非线性模型，而核函数便是其中一种。



我们取三个点 $l^{(1)}、l^{(2)}、l^{(3)}$，

<img src="https://img2022.cnblogs.com/blog/1754203/202205/1754203-20220519230037969-188004899.png" alt="image-20220519230038832" style="zoom:33%;" />

对于给定的x，我们定义新的特征 $f_i=similarity(x,\;l^{(i)})=exp(-\frac{||x-l^{(i)}||^2}{2\sigma^2})=exp(-\frac{\sum_{j=1}^n(x_j-l_j^{(i)})^2}{2\sigma^2})$

这里的相似度函数similarity，被称为核函数，而exp(…)被称为高斯核函数

此时

当 $x\approx l^{(i)}:f_i\approx exp(-\frac{0}{2\sigma^2})\approx 1$

当 $x 远离 l^{(i)}:f_i\approx exp(-\frac{(large\;number)^2}{2\sigma^2})\approx 0$

 借此我们便能画出决策边界

![image-20220519230810742](https://img2022.cnblogs.com/blog/1754203/202205/1754203-20220519230810049-1650241688.png)

**如何选取 $l$ 呢？**

![image-20220519231738506](https://img2022.cnblogs.com/blog/1754203/202205/1754203-20220519231737480-354799412.png)

也就是将每个$l$放在每个数据的位置

对于单个样本$x$：

令$f_i=similarity(x,\;l^{(i)})$，便能得到向量$f=\begin{bmatrix} f_0 \\ f_1 \\ f_2 \\ \cdots \\ f_m\end{bmatrix}$

对于训练集：

令$f_i^{(i)}=similarity(x^{(i)},\;l^{(i)})$，便能得到向量$f^{(i)}=\begin{bmatrix} f^{(i)}_0 \\ f^{(i)}_1 \\ f^{(i)}_2 \\ \cdots \\ f^{(i)}_m\end{bmatrix}$



### SVM with Kernels

 对于给定的 $x$，我们计算 $f \in \R^{m+1}$

并且 Predict “y = 1” if $\theta^Tf>0$

![image-20220519233323067](https://img2022.cnblogs.com/blog/1754203/202205/1754203-20220519233322043-1370949867.png)

注意这里最后一项是 $j从1到m，相当于n=m$

一般计算 $\sum_j\theta_j^2$，我们用$\theta^T\theta$，不过更多的是 $\theta^TM\theta$，M是一个矩阵，取决于核函数，这样可以用来优化计算效率

![image-20220519233848128](https://img2022.cnblogs.com/blog/1754203/202205/1754203-20220519233847219-1328380760.png)