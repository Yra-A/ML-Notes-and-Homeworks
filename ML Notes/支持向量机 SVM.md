此系列笔记来源于

**Coursera上吴恩达老师的机器学习课程**

****

## 支持向量机

### 优化目标

![image-20220517234008116](https://img2022.cnblogs.com/blog/1754203/202205/1754203-20220517234008604-493072839.png)

图中 y = 1时 是 want $\theta^Tx\ge1$

图中 y = 0时 是 want $\theta^Tx\le-1$

我们将逻辑回归中的曲线，变为紫色的线，由一条斜的直线和一条水平直线组成，并将其对应的函数变为$cost_0(z)$ 和 $cost_1(z)$

![image-20220517234216091](https://img2022.cnblogs.com/blog/1754203/202205/1754203-20220517234220155-938609704.png)

在逻辑回归中，这个是我们要优化的代价函数，我们按上面所述的更改函数，同时去掉 $\lambda$ 部分，并在前一项之前加常数 C作为更改权重的参数。

即：$A + \lambda B\;\rightarrow\;CA+B$

C可以理解为$\frac{1}{\lambda}$

由此我们便能得到SVM中我们所要优化的函数：

![image-20220517234541610](https://img2022.cnblogs.com/blog/1754203/202205/1754203-20220517234541818-356333264.png)

这里的假设函数:
$$
h_{\Theta}(x)=\left\{
\begin{aligned}
1,\;\;\Theta^TX\ge0 \\
0,\;\;\Theta^TX<0
\end{aligned}
\right.
$$


### 大间隔分类器

对于SVM的假设函数，我们当然希望其cost函数为0，由图像可知，

![image-20220518001633171](https://img2022.cnblogs.com/blog/1754203/202205/1754203-20220518001633305-1242657814.png)

SVM是一种大间隔分类器，即其所绘决策边界与各类之间有着较大的间隔

例：图中黑线，而非绿线和紫线

![image-20220518001804219](https://img2022.cnblogs.com/blog/1754203/202205/1754203-20220518001804445-990375280.png)

由于此特性，SVM具有鲁棒性（面对异常时的具有更高的稳定性、容错率）



#### 数学原理：

因为在满足上面的我们希望的条件时，代价函数的前半部分基本等于 0，因此只用讨论第二项 $\frac{1}{2}\sum^n_{j=1}\theta_j^2$

首先我们可以将公式 用线性代数的方法如下表示，$p^{(i)}$是$x^{(i)}$在向量$\theta$上的投影，同时我们进行简单化，令 $\theta_0 = 0$

![image-20220519153352563](https://img2022.cnblogs.com/blog/1754203/202205/1754203-20220519153353411-726312694.png)

首先，向量$\theta$因为作为系数，且方程等于0，所以他与决策边界垂直（即点积为0）

蓝色的即为向量$\theta$，绿色的是决策边界。

我们的目的是最小化 $||\theta||$

那么根据图一，此时$p^{(i)}$很小，因此为了满足条件，$||\theta||$就很大，则矛盾

因此，如图二，此时决策边界是大间距的，向量$\theta$与x轴平行 此时$p^{(i)}$比起图一显然增大了许多，对应的则$||\theta||$就可以变得更小了

