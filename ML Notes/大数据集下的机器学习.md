此系列笔记来源于

**Coursera上吴恩达老师的机器学习课程**

****

## 随机梯度下降（Stochastic gradient descent）

在处理大数据时，即m的值非常大，此时我们算法的计算成本将会非常大，对于之前的梯度下降算法，我们称为批量梯度下降算法（Batch gradient descent）。此时，我们就要用到随机梯度下降算法SGD，以提高运算效率。

### 算法过程：

![image-20220528125234070](https://img2022.cnblogs.com/blog/1754203/202205/1754203-20220528125235125-1106070629.png)

SGD 在每一次计算之后便更新参数 $\theta$，而不需要首先将所有的训练集求和，在降算法还没有完成一次迭代时，SGD 便已经走出了很远。但是这样的算法存在的问题是，不是每一步都是朝着“正确”的方向迈出的。因此算法虽然会逐渐走向全局最小值的位置，但是可能无法站到那个最小值的那一点，而是在最小值点附近徘徊。

![img](https://i.loli.net/2018/12/02/5c031b16b2ae2.png)

### 随机梯度下降算法的收敛性

在批量梯度下降中，我们可以令代价函数 J 为迭代次数的函数，绘制图表，根据图表来判断梯度下降是否收敛。但是，在大规模的训练集的情况下，这是不现实的，因为计算代价太大了。

在随机梯度下降中，我们在每一次更新θ之前都计算一次代价，然后每 X 次迭代后，求出这 X 次对训练实例计算代价的平均值，然后绘制这些平均值与 X 次迭代的次数之间的函数图表。

![img](https://i.loli.net/2018/12/02/5c031d4d11b6c.png)

如果得到一个颠簸不平但是不会明显减少的函数图像（如上面左下图中蓝线所示）。我们可以增加 X 来使得函数更加平缓，也许便能看出下降的趋势（如上面左下图中红线所示）；或者可能函数图表仍然是颠簸不平且不下降的（如洋红色线所示），那么我们的模型本身可能存在一些错误。

如果我们得到的曲线如上面右下方所示，不断地上升，那么我们可能会需要选择一个较小的学习率 $\alpha$。也可以令学习率随着迭代次数的增加而减小，例如令：$\alpha=\frac{const1}{iterationNumber+cosnt2}$

随着我们不断地靠近全局最小值，通过减小学习率，我们迫使算法收敛而非在最小值附近徘徊。但是通常我们不需要这样做便能有非常好的效果了，对 α 进行调整所耗费的计算通常不值得。

## 小批量梯度下降（Mini-Batch gradient descent）

小批量梯度下降，是对批量梯度下降以及随机梯度下降的一个折中办法。其思想是：每次迭代 使用 **batch_size** 个样本来对参数进行更新。

![image-20220529013655798](https://img2022.cnblogs.com/blog/1754203/202205/1754203-20220529013654913-1878496790.png)

这里我们假设 batchsize = 10，样本数 m = 1000。   

 伪代码形式为：

![image-20220529013717547](https://img2022.cnblogs.com/blog/1754203/202205/1754203-20220529013716474-527217302.png)

**优点**：

1.  通过矩阵运算，每次在一个batch上优化神经网络参数并不会比单个数据慢太多。
2.  每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果。(比如上例中的30W，设置batch_size=100时，需要迭代3000次，远小于SGD的30W次)
3.  可实现并行化。

**缺点**：

1.  batch_size的不当选择可能会带来一些问题。

## 在线学习

在线学习算法指的是对数据流而非离线的静态数据集的学习。许多在线网站都有持续不断的用户流，对于每一个用户，网站希望能在不将数据存储到数据库中便顺利地进行算法学习。

假使我们正在经营一家物流公司，每当一个用户询问从地点 A 至地点 B 的快递费用时，我们给用户一个报价，该用户可能选择接受（y = 1）或不接受（y = 0）。

现在，我们希望构建一个模型，来预测用户接受报价使用我们的物流服务的可能性。因此报价是我们的一个特征，其他特征为距离，起始地点，目标地点以及特定的用户数据。模型的输出是 p(y=1)。

在线学习的算法与随机梯度下降算法有些类似，我们对单一的实例进行学习，而非对一个提前定义的训练集进行循环。

![img](https://i.loli.net/2018/12/02/5c031f7767133.png)

一旦对一个数据的学习完成了，我们便可以丢弃该数据，不需要再存储它了。这种方式的好处在于，我们的算法可以很好的适应用户的倾向性，算法可以针对用户的当前行为不断地更新模型以适应该用户。

在线学习的一个优点就是，如果你有一个变化的用户群，又或者你在尝试预测的事情，在缓慢变化，就像你的用户的品味在缓慢变化，这个在线学习算法，可以慢慢地调试你所学习到的假设，将其调节更新到最新的用户行为。

## Map Reduce 和 数据并行

Map Reduce和数据并行对于大规模机器学习问题而言是非常重要的概念。

如果我们用批量梯度下降算法来求解大规模数据集的最优解，我们需要对整个训练集进行循环，计算偏导数和代价，再求和，计算代价非常大。

如果我们能够将我们的数据集分配给多台计算机，让每一台计算机处理数据集的一个子集，然后我们将计算的结果汇总然后再求和。这样的方法叫做Map Reduce。

![img](https://i.loli.net/2018/12/02/5c032040ec21d.png)

具体而言，如果任何学习算法能够表达为，对训练集的函数的求和，那么便能将这个任务分配给多台计算机（或者同一台计算机的不同 CPU 核心），以达到加速处理的目的。

例如，我们有 400 个训练实例，我们可以将批量梯度下降的求和任务分配给 4 台计算机进行处理：

![img](https://i.loli.net/2018/12/02/5c032078d2e92.png)

很多高级的线性代数函数库已经能够利用多核 CPU 的多个核心来并行地处理矩阵运算，这也是算法的向量化实现如此重要的缘故（比调用循环快）。

